{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-4-rccLEuOO7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxb1L6quvCig"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class KNN:\n",
        "     k:int=3\n",
        "     def fit(self,x,y):\n",
        "      self.x=x\n",
        "      self.y=y\n",
        "     def predict(self,X_test):\n",
        "      #print((self.x-X_test)**2)\n",
        "      dis=np.sqrt((self.x-X_test)**2).sum(axis=1)\n",
        "      print(dis)\n",
        "      ind=np.argsort(dis)[:self.k]\n",
        "      print(ind)\n",
        "      print(np.bincount(self.y[ind]))\n",
        "      print(np.argmax(np.bincount(self.y[ind])))\n",
        "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
        "y = np.array([0, 1, 0, 1])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
        "k=KNN()\n",
        "k.fit(X_train,y_train)\n",
        "k.predict(X_test)\n",
        "print(X_train,y_train)\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryOXMdtIitS1"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/badminton_dataset.csv')\n",
        "y=df.iloc[:,-1]\n",
        "X=df.iloc[:,:-1]\n",
        "print(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4vORKbj7kap"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ihkRjzy3jX8"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class NaiveBayes:\n",
        "  def fit(self,x,y):\n",
        "    self.x=x.values\n",
        "    self.y=y.values.ravel()\n",
        "    self.data=pd.concat([x,y],axis=1).values\n",
        "    #print(self.data)\n",
        "    self.condprob=[]\n",
        "    print(self.x.shape)\n",
        "    for i in np.unique(y):\n",
        "      #print(i)\n",
        "      setattr(self, i ,len(self.y[self.y==i]))\n",
        "      setattr(self, f\"prob_{i}\",len(self.y[self.y==i])/len(y))\n",
        "      print(i,getattr(self,i))\n",
        "      print(f\"prob_{i}\",getattr(self, f\"prob_{i}\"))\n",
        "    for i in range(self.x.shape[1]):\n",
        "          ip=self.x[:,i]\n",
        "          #featureclassprobs={}\n",
        "          for j in np.unique(ip):\n",
        "            featureprobs={}\n",
        "            for k in np.unique(y):\n",
        "                fe=np.sum((self.data[:,i]==j)&(self.data[:,-1]==k))\n",
        "                #featureprobs[j]=((fe)/(getattr(self,k)))\n",
        "                setattr(self,f\"{j}_{k}\",((fe)/(getattr(self,k))))\n",
        "                print(f\"{j}_{k}\",getattr(self,f\"{j}_{k}\"))\n",
        "            #featureclassprobs[k]=featureprobs\n",
        "          #self.condprob.append(featureclassprobs)\n",
        "          print(ip)\n",
        "          print(self.condprob)\n",
        "  def predict(self,X_test):\n",
        "      y_pred=np.array([])\n",
        "      for X in X_test.values:\n",
        "         print(X)\n",
        "         totalres=[]\n",
        "         result=1\n",
        "         for j in np.unique(y):\n",
        "            res=np.array([])\n",
        "            for i in range(len(X)):\n",
        "             #print(i,self.condprob[i][X[i]])\n",
        "                res=np.append(res,getattr(self,f\"{X[i]}_{j}\"))#self.condprob[i][X[i]]\n",
        "                result=res\n",
        "            result=np.append(result,[getattr(self,f\"prob_{j}\")])\n",
        "            totalres.append({j:np.prod(result)})\n",
        "         print(totalres)\n",
        "         #sum=0\n",
        "         dic={}\n",
        "         lis=[]\n",
        "         for item in totalres:\n",
        "            dic.update(item)\n",
        "         #print(dic)\n",
        "\n",
        "         total=sum([i for i in dic.values()])\n",
        "         #print(\"hi\",total)\n",
        "         lis=[(label,(value/total)) for label,value in dic.items()]\n",
        "         #print(lis)\n",
        "         lis.sort(key=lambda x:x[1],reverse=True)\n",
        "         print(lis[0][0])\n",
        "         y_pred=np.append(y_pred,[lis[0][0]])\n",
        "      return y_pred\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1,random_state=5)\n",
        "n=NaiveBayes()\n",
        "n.fit(X,y)\n",
        "print(n.condprob)\n",
        "y_pred=n.predict(X_test)\n",
        "print(accuracy_score(y_pred,y_test))\n",
        "#print(X['astigmatic'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDRegressor:\n",
        "\n",
        "    def __init__(self,learning_rate=0.01,epochs=100):\n",
        "\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self,X_train,y_train):\n",
        "        # init your coefs\n",
        "        self.intercept_ = 0\n",
        "        self.coef_ = np.ones(X_train.shape[1])\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "            for j in range(X_train.shape[0]):\n",
        "                idx = np.random.randint(0,X_train.shape[0])\n",
        "\n",
        "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
        "\n",
        "                intercept_der = -2 * (y_train[idx] - y_hat)\n",
        "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
        "\n",
        "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
        "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
        "\n",
        "        print(self.intercept_,self.coef_)\n",
        "\n",
        "    def predict(self,X_test):\n",
        "        return np.dot(X_test,self.coef_) + self.intercept_\n",
        "\n",
        "import random\n",
        "\n",
        "class MBGDRegressor:\n",
        "\n",
        "    def __init__(self,batch_size,learning_rate=0.01,epochs=100):\n",
        "\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def fit(self,X_train,y_train):\n",
        "        # init your coefs\n",
        "        self.intercept_ = 0\n",
        "        self.coef_ = np.ones(X_train.shape[1])\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            for j in range(int(X_train.shape[0]/self.batch_size)):\n",
        "\n",
        "                idx = random.sample(range(X_train.shape[0]),self.batch_size)\n",
        "\n",
        "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
        "                #print(\"Shape of y_hat\",y_hat.shape)\n",
        "                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n",
        "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
        "\n",
        "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
        "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
        "\n",
        "        print(self.intercept_,self.coef_)\n",
        "\n",
        "    def predict(self,X_test):\n",
        "        return np.dot(X_test,self.coef_) + self.intercept_\n",
        "\n",
        "class GDRegressor:\n",
        "\n",
        "    def __init__(self,learning_rate=0.01,epochs=100):\n",
        "\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self,X_train,y_train):\n",
        "        # init your coefs\n",
        "        self.intercept_ = 0\n",
        "        self.coef_ = np.ones(X_train.shape[1])\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "            # update all the coef and the intercept\n",
        "            y_hat = np.dot(X_train,self.coef_) + self.intercept_\n",
        "            #print(\"Shape of y_hat\",y_hat.shape)\n",
        "            intercept_der = -2 * np.mean(y_train - y_hat)\n",
        "            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
        "\n",
        "            coef_der = -2 * np.dot((y_train - y_hat),X_train)/X_train.shape[0]\n",
        "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
        "\n",
        "        print(self.intercept_,self.coef_)\n",
        "\n",
        "    def predict(self,X_test):\n",
        "        return np.dot(X_test,self.coef_) + self.intercept_"
      ],
      "metadata": {
        "id": "zT3nvBrgWjr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass,field\n",
        "@dataclass\n",
        "class Node:\n",
        "  feature:str=None\n",
        "  threshold:str=None\n",
        "  gain:int=None\n",
        "  sample:int=None\n",
        "  values:dict=None\n",
        "  left:Node=None\n",
        "  right:Node=None\n",
        "  opclass:str=None\n",
        "  isleaf:bool=False\n",
        "@dataclass\n",
        "class DecisionTreeClassifier:\n",
        "            root=None\n",
        "            def fit(self,X,y):\n",
        "                data=pd.concat([X,y],ignore_index=False,axis=1)\n",
        "                self.root=self.buildtree(data)\n",
        "\n",
        "            def buildtree(self,data):\n",
        "                if len(data.iloc[:,-1].unique())==1:\n",
        "                        opclass=data.iloc[:,-1].unique()[0]\n",
        "                        sample=data.iloc[:,-1].shape[0]\n",
        "                        return Node(gain=0,values={opclass:sample},opclass=opclass,sample=sample,isleaf=True)\n",
        "\n",
        "\n",
        "                print(data)\n",
        "\n",
        "                leftdata,rightdata,bestgain,bestfeature,bestthreshold,resultvalue,opclass,sample=self.bestsplit(data)\n",
        "\n",
        "                left=self.buildtree(leftdata)\n",
        "                right=self.buildtree(rightdata)\n",
        "                return Node(feature=bestfeature,threshold=bestthreshold,gain=bestgain,values=resultvalue,opclass=opclass,sample=sample,left=left,right=right)\n",
        "            def bestsplit(self,data):\n",
        "                print(data)\n",
        "                op=data.iloc[:,-1].value_counts().to_dict()\n",
        "                print(op)\n",
        "                bestgain=-1\n",
        "                global bestfeature\n",
        "                global bestthreshold\n",
        "                global resultvalue\n",
        "                opclass=None\n",
        "                sample=None\n",
        "                for col in data.columns:\n",
        "\n",
        "                  if col != data.columns[-1]:\n",
        "                        threshold=data[col].unique()\n",
        "                        for thres in threshold:\n",
        "                            print(thres)\n",
        "                            df=pd.crosstab(index=data[data[col]==thres][col],columns=data[data[col]==thres][data.columns[-1]],margins=True,margins_name='Sum')\n",
        "                            df1=pd.crosstab(index=data[data[col]!=thres][col],columns=data[data[col]!=thres][data.columns[-1]],margins=True,margins_name='Sum')\n",
        "                            dic={}\n",
        "                            dic1={}\n",
        "                            value={}\n",
        "                            valuedash={}\n",
        "                            coln=None\n",
        "                            thresh=None\n",
        "                            print('check df empty',df.empty)\n",
        "                            print('check df1 empty',df1.empty)\n",
        "                            for i in df.columns:\n",
        "                                if i!='Sum':\n",
        "                                    value[i]=df.loc['Sum',i]\n",
        "                                    df[f\"prob_{i}\"]=df.loc['Sum',i]/df.loc['Sum','Sum']\n",
        "                                    dic[f\"prob_{thres}_{i}\"]=df.loc['Sum',i]/df.loc['Sum','Sum']\n",
        "                            if (not df1.empty):\n",
        "                              for j in df1.columns:\n",
        "                                if j!='Sum':\n",
        "\n",
        "                                    valuedash[j]=df1.loc['Sum',j]\n",
        "                                    df1[f\"prob_{j}\"]=df1.loc['Sum',j]/df1.loc['Sum','Sum']\n",
        "                                    dic1[f\"prob_not{thres}_{j}\"]=df1.loc['Sum',j]/df1.loc['Sum','Sum']\n",
        "                            if df1.empty:\n",
        "                                gain,coln,thresh=self.informationgain(data,op,df,df1,dic,dic1,col,thres)\n",
        "                            else:\n",
        "                                gain,coln,thresh=self.informationgain(data,op,df,df1,dic,dic1,col,thres)\n",
        "                            print(\"gain\",gain)\n",
        "                            print(\"bestgain\",bestgain)\n",
        "\n",
        "                            if gain>bestgain:\n",
        "                                print(gain,bestgain)\n",
        "                                bestgain=gain\n",
        "                                bestfeature=coln\n",
        "                                bestthreshold=thresh\n",
        "                                print(\"bestfeature\",bestfeature)\n",
        "                                print(\"bestthreshold\",bestthreshold)\n",
        "                                if df1.empty:\n",
        "                                   for key in value:\n",
        "                                    resultvalue[key]=value[key]\n",
        "                                    opclass=sorted(resultvalue.items(),key=lambda x:x[1],reverse=True )[0][0]\n",
        "                                    sample=data.shape[0]\n",
        "                                else:\n",
        "                                    for o in op.keys():\n",
        "                                            if o not in value.keys():\n",
        "                                                value[o]=0\n",
        "                                            if o not in valuedash.keys():\n",
        "                                                valuedash[o]=0\n",
        "                                    print(value,valuedash)\n",
        "                                    resultvalue={}\n",
        "                                    for key in value:\n",
        "                                        resultvalue[key]=value[key]+valuedash[key]\n",
        "                                    opclass=sorted(resultvalue.items(),key=lambda x:x[1],reverse=True )[0][0]\n",
        "                                    sample=data.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                leftdata=data[data[bestfeature]==bestthreshold]\n",
        "                rightdata=data[data[bestfeature]!=bestthreshold]\n",
        "                return(leftdata,rightdata,bestgain,bestfeature,bestthreshold,resultvalue,opclass,sample)\n",
        "            def informationgain(self,data,op,df,df1,dic,dic1,col,thres):\n",
        "                print(df)\n",
        "                print()\n",
        "                print(df1)\n",
        "                print()\n",
        "                weightedsump=df.loc['Sum','Sum']\n",
        "                if df1.empty and len(dic1)==0:\n",
        "                    totalsum=np.sum(list(op.values()))\n",
        "                    dataentropy=self.entropy(list(np.array(list(op.values()))/totalsum))\n",
        "                    e=self.entropy(list(dic.values()))\n",
        "                    return ((dataentropy-(weightedsump/totalsum)*e),col,thres)\n",
        "                weightedsumpdash=df1.loc['Sum','Sum']\n",
        "                p=[]\n",
        "                pdash=[]\n",
        "                print(dic)\n",
        "\n",
        "\n",
        "                totalsum=np.sum(list(op.values()))\n",
        "                dataentropy=self.entropy(list(np.array(list(op.values()))/totalsum))\n",
        "                e=self.entropy(list(dic.values()))\n",
        "                edash=self.entropy(list(dic1.values()))\n",
        "                print(dataentropy,e,edash)\n",
        "                return ((dataentropy-(weightedsump/totalsum)*e-(weightedsumpdash/totalsum)*edash),col,thres)\n",
        "\n",
        "\n",
        "\n",
        "            def entropy(self,prob):\n",
        "                return np.sum([-(i)*(np.log2(i))for i in prob if i>0])\n",
        "\n",
        "            def predict(self, X):\n",
        "                   print(self._traverse_tree(X, self.root))\n",
        "                   #return np.array([self._traverse_tree(x, self.root) for x in X])\n",
        "\n",
        "            def _traverse_tree(self, x, node):\n",
        "                if node.isleaf:\n",
        "                    return node.opclass\n",
        "\n",
        "                if x[node.feature] == node.threshold:\n",
        "                    return self._traverse_tree(x, node.left)\n",
        "                return self._traverse_tree(x, node.right)\n",
        "\n",
        "\n",
        "import graphviz\n",
        "\n",
        "def export_graphviz(root, filename='tree.png'):\n",
        "    graph = graphviz.Digraph(format='png')\n",
        "    recurse_tree(root, graph)\n",
        "    graph.render(filename, cleanup=True)\n",
        "    return filename\n",
        "\n",
        "def recurse_tree(node, graph):\n",
        "    if node.isleaf:\n",
        "        graph.node(str(id(node)), label=f\"{node.opclass}\\nSamples = {node.sample}\\nGain = {node.gain}\", fillcolor=\"#ffffff\")\n",
        "    else:\n",
        "        graph.node(str(id(node)), label=f\"{node.feature}== {node.threshold}\\nSamples = {node.sample}\\nGain = {node.gain}\", fillcolor=\"#ffffff\")\n",
        "        recurse_tree(node.left, graph)\n",
        "        recurse_tree(node.right, graph)\n",
        "        graph.edge(str(id(node)), str(id(node.left)), label=\"True\")\n",
        "        graph.edge(str(id(node)), str(id(node.right)), label=\"False\")\n",
        "\n",
        "dtc=DecisionTreeClassifier()\n",
        "dtc.fit(X,y)\n",
        "root=dtc.root\n",
        "export_graphviz(root)"
      ],
      "metadata": {
        "id": "cHsVQu-S0rwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "  def __init__(self,tree):\n",
        "    self.root=None\n",
        "    self.tree=tree\n",
        "    self.queue=[]\n",
        "\n",
        "  def valueCountsDict(self,df,column):\n",
        "    valueCounts=np.unique(df[column],return_counts=True)\n",
        "    valueCountsDict = dict(zip(valueCounts[0],valueCounts[1]))\n",
        "    return valueCountsDict\n",
        "\n",
        "  def entropy(self,df,column):\n",
        "    entropy=0\n",
        "    tempdf=pd.DataFrame(columns=self.valueCountsDict(df,df.iloc[:,-1].name).keys(),index=self.valueCountsDict(df,column).keys())\n",
        "    for x in self.valueCountsDict(df,column).keys():\n",
        "      for y in self.valueCountsDict(df,df.iloc[:,-1].name).keys():\n",
        "        tempdf.loc[x,y]=len(df[(df[column]==x) & (df[df.iloc[:,-1].name]==y)].index)\n",
        "    valueCounts=self.valueCountsDict(df,column)\n",
        "    targetValueCounts=self.valueCountsDict(df,df.iloc[:,-1].name)\n",
        "    total=len(df.index)\n",
        "    for category in valueCounts.keys():\n",
        "      prob=valueCounts[category]/total\n",
        "      infoGain=0\n",
        "      for targetCategory in targetValueCounts.keys():\n",
        "        infoprob=tempdf.loc[category,targetCategory]/tempdf.loc[category].sum()\n",
        "        if infoprob!=0:\n",
        "          infoGain-=(infoprob*np.log2(infoprob))\n",
        "      entropy+=prob*infoGain\n",
        "    return entropy\n",
        "\n",
        "  def informationGain(self,df):\n",
        "    total=len(df.index)\n",
        "    column=df.iloc[:,-1].name\n",
        "    valueCounts=self.valueCountsDict(df,column)\n",
        "    infoGain=0\n",
        "    for category in valueCounts.keys():\n",
        "      prob=valueCounts[category]/total\n",
        "      if prob!=0:\n",
        "        infoGain-=(prob*np.log2(prob))\n",
        "    return infoGain\n",
        "\n",
        "  def getSplittingFeature(self,df):\n",
        "    featureEntropyDict={}\n",
        "    featureGainDict={}\n",
        "    for feature in df.iloc[:,0:-1].columns:\n",
        "      featureEntropyDict[feature]=self.entropy(df,feature)\n",
        "      featureGainDict[feature]=self.informationGain(df)-self.entropy(df,feature)\n",
        "    maxGainValue=max(featureGainDict.values())\n",
        "    for splittingFeature,gainValue in featureGainDict.items():\n",
        "      if gainValue==maxGainValue:\n",
        "        return splittingFeature,gainValue,featureEntropyDict[splittingFeature]\n",
        "\n",
        "  def buildTree(self, node=None):\n",
        "          if node is None:\n",
        "              df = self.X\n",
        "              informationGain = self.informationGain(df)\n",
        "              splittingFeature, gainValue, entropyValue = self.getSplittingFeature(df)\n",
        "              root = Node(value=splittingFeature, informationGain=informationGain,\n",
        "                          entropy=entropyValue, gain=gainValue)\n",
        "              self.tree.node(root.value)\n",
        "              root.setDefaultLinks(self.X)\n",
        "              self.root = root\n",
        "              self.queue.append(root)\n",
        "          else:\n",
        "              for link, value in node.linksDict.items():\n",
        "                  if value is None:\n",
        "                      df = self.X\n",
        "                      dfConstraints = node.dfConstraints.copy()\n",
        "                      dfConstraints[node.value] = link\n",
        "                      for key, value in dfConstraints.items():\n",
        "                          df = df[df[key] == value]\n",
        "                      df = df.drop(dfConstraints.keys(), axis=1)\n",
        "                      if not df.empty:\n",
        "                          informationGain = self.informationGain(df)\n",
        "                          if informationGain == 0:\n",
        "                              nodeValue = df.iloc[0, -1]\n",
        "                              newnode = Node(value=nodeValue)\n",
        "                              node.linksDict[link] = newnode\n",
        "                              self.tree.node(link, label=newnode.value)\n",
        "                              self.tree.edge(node.value, link, label=link)\n",
        "                          else:\n",
        "                              splittingFeature, gainValue, entropyValue = self.getSplittingFeature(df)\n",
        "                              newnode = Node(value=splittingFeature, informationGain=informationGain,\n",
        "                                            entropy=entropyValue, gain=gainValue, dfConstraints=dfConstraints)\n",
        "                              newnode.setDefaultLinks(self.X)\n",
        "                              newnode.dfConstraints = dfConstraints\n",
        "                              node.linksDict[link] = newnode\n",
        "                              self.tree.node(newnode.value)\n",
        "                              self.tree.edge(node.value, newnode.value, label=link)\n",
        "                              self.queue.append(newnode)\n",
        "\n",
        "  def fit(self,X,y):\n",
        "    self.X=X\n",
        "    self.y=y\n",
        "    self.df=X\n",
        "    self.df[y.name]=y\n",
        "    self.buildTree()\n",
        "    while self.queue != []:\n",
        "      node=self.queue.pop(0)\n",
        "      # print(node.value,\":\")\n",
        "      self.buildTree(node)\n",
        "    self.tree.render('DecisionTree_predict.dot', format='png')\n",
        "\n",
        "  def predict(self, X):\n",
        "      predictions = []\n",
        "      for _, row in X.iterrows():\n",
        "          current_node = self.root\n",
        "          while current_node.linksDict:\n",
        "              feature_value = row[current_node.value]\n",
        "              current_node = current_node.linksDict[feature_value]\n",
        "          predictions.append(current_node.value)\n",
        "      return predictions\n",
        "tree=graphviz.Digraph()\n",
        "t=DecisionTree(tree=tree)\n",
        "t.fit(X,y)"
      ],
      "metadata": {
        "id": "-wMyFWAJRA1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "  def __init__(self,value,informationGain=None,entropy=None,gain=None,dfConstraints={},class_distribution={}):\n",
        "    self.value=value\n",
        "    self.informationGain=informationGain\n",
        "    self.entropy=entropy\n",
        "    self.gain=gain\n",
        "    self.linksDict={}\n",
        "    self.isLeaf=False\n",
        "    self.dfConstraints=dfConstraints\n",
        "    self.class_distribution = class_distribution\n",
        "  def setDefaultLinks(self,df):\n",
        "    for category in df[self.value].unique():\n",
        "      self.linksDict[category]=None\n",
        "\n",
        "class DecisionTree:\n",
        "  def __init__(self,tree,alpha=0.2):\n",
        "    self.root=None\n",
        "    self.tree=tree\n",
        "    self.queue=[]\n",
        "    self.alpha = alpha\n",
        "\n",
        "  def valueCountsDict(self,df,column):\n",
        "    valueCounts=np.unique(df[column],return_counts=True)\n",
        "    valueCountsDict = dict(zip(valueCounts[0],valueCounts[1]))\n",
        "    return valueCountsDict\n",
        "\n",
        "  def entropy(self,df,column):\n",
        "    entropy=0\n",
        "    tempdf=pd.DataFrame(columns=self.valueCountsDict(df,df.iloc[:,-1].name).keys(),index=self.valueCountsDict(df,column).keys())\n",
        "    for x in self.valueCountsDict(df,column).keys():\n",
        "      for y in self.valueCountsDict(df,df.iloc[:,-1].name).keys():\n",
        "        tempdf.loc[x,y]=len(df[(df[column]==x) & (df[df.iloc[:,-1].name]==y)].index)\n",
        "    valueCounts=self.valueCountsDict(df,column)\n",
        "    targetValueCounts=self.valueCountsDict(df,df.iloc[:,-1].name)\n",
        "    total=len(df.index)\n",
        "    for category in valueCounts.keys():\n",
        "      prob=valueCounts[category]/total\n",
        "      infoGain=0\n",
        "      for targetCategory in targetValueCounts.keys():\n",
        "        infoprob=tempdf.loc[category,targetCategory]/tempdf.loc[category].sum()\n",
        "        if infoprob!=0:\n",
        "          infoGain-=(infoprob*np.log2(infoprob))\n",
        "      entropy+=prob*infoGain\n",
        "    return entropy\n",
        "\n",
        "  def informationGain(self,df):\n",
        "    total=len(df.index)\n",
        "    column=df.iloc[:,-1].name\n",
        "    valueCounts=self.valueCountsDict(df,column)\n",
        "    infoGain=0\n",
        "    for category in valueCounts.keys():\n",
        "      prob=valueCounts[category]/total\n",
        "      if prob!=0:\n",
        "        infoGain-=(prob*np.log2(prob))\n",
        "    return infoGain\n",
        "\n",
        "  def getSplittingFeature(self,df):\n",
        "    featureEntropyDict={}\n",
        "    featureGainDict={}\n",
        "    for feature in df.iloc[:,0:-1].columns:\n",
        "      featureEntropyDict[feature]=self.entropy(df,feature)\n",
        "      featureGainDict[feature]=self.informationGain(df)-self.entropy(df,feature)\n",
        "    maxGainValue=max(featureGainDict.values())\n",
        "    for splittingFeature,gainValue in featureGainDict.items():\n",
        "      if gainValue==maxGainValue:\n",
        "        return splittingFeature,gainValue,featureEntropyDict[splittingFeature]\n",
        "\n",
        "  def buildTree(self, node=None):\n",
        "        if node is None:\n",
        "            df = self.X\n",
        "            informationGain = self.informationGain(df)\n",
        "            splittingFeature, gainValue, entropyValue = self.getSplittingFeature(df)\n",
        "            root = Node(value=splittingFeature, informationGain=informationGain,\n",
        "                        entropy=entropyValue, gain=gainValue,\n",
        "                        class_distribution=self.valueCountsDict(df, df.iloc[:, -1].name))  # Pass class distribution\n",
        "            self.tree.node(root.value)\n",
        "            root.setDefaultLinks(self.X)\n",
        "            self.root = root\n",
        "            self.queue.append(root)\n",
        "        else:\n",
        "            for link, value in node.linksDict.items():\n",
        "                if value is None:\n",
        "                    df = self.X\n",
        "                    dfConstraints = node.dfConstraints.copy()\n",
        "                    dfConstraints[node.value] = link\n",
        "                    for key, value in dfConstraints.items():\n",
        "                        df = df[df[key] == value]\n",
        "                    df = df.drop(dfConstraints.keys(), axis=1)\n",
        "                    if not df.empty:\n",
        "                        informationGain = self.informationGain(df)\n",
        "                        if informationGain == 0:\n",
        "                            nodeValue = df.iloc[0, -1]\n",
        "                            newnode = Node(value=nodeValue)\n",
        "                            node.linksDict[link] = newnode\n",
        "                            self.tree.node(link, label=newnode.value)\n",
        "                            self.tree.edge(node.value, link, label=link)\n",
        "                        else:\n",
        "                            splittingFeature, gainValue, entropyValue = self.getSplittingFeature(df)\n",
        "                            newnode = Node(value=splittingFeature, informationGain=informationGain,\n",
        "                                           entropy=entropyValue, gain=gainValue,\n",
        "                                           dfConstraints=dfConstraints,\n",
        "                                           class_distribution=self.valueCountsDict(df, df.iloc[:, -1].name))  # Pass class distribution\n",
        "                            newnode.setDefaultLinks(self.X)\n",
        "                            newnode.dfConstraints = dfConstraints\n",
        "                            node.linksDict[link] = newnode\n",
        "                            self.tree.node(newnode.value)\n",
        "                            self.tree.edge(node.value, newnode.value, label=link)\n",
        "                            self.queue.append(newnode)\n",
        "  # def fit(self,X,y):\n",
        "  #   self.X=X\n",
        "  #   self.y=y\n",
        "  #   self.df=X\n",
        "  #   self.df[y.name]=y\n",
        "  #   self.buildTree()\n",
        "  #   while self.queue != []:\n",
        "  #     node=self.queue.pop(0)\n",
        "  #     # print(node.value,\":\")\n",
        "  #     self.buildTree(node)\n",
        "  #   self.tree.render('DecisionTree.dot', format='png')\n",
        "\n",
        "  def predict(self, X):\n",
        "      predictions = []\n",
        "      for _, row in X.iterrows():\n",
        "          current_node = self.root\n",
        "          while current_node.linksDict:\n",
        "              feature_value = row[current_node.value]\n",
        "              current_node = current_node.linksDict[feature_value]\n",
        "          predictions.append(current_node.value)\n",
        "      return predictions\n",
        "\n",
        "  def fit(self, X, y,alpha=0.2):\n",
        "        self.alpha = alpha\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.df = X\n",
        "        self.df[y.name] = y\n",
        "        self.buildTree()\n",
        "        while self.queue:\n",
        "            node = self.queue.pop(0)\n",
        "            self.buildTree(node)\n",
        "        # Perform cost complexity pruning\n",
        "        self.cost_complexity_pruning(self.root)\n",
        "        self.tree.render('Pruned_DecisionTree', format='png')\n",
        "\n",
        "  def cost_complexity_pruning(self, node):\n",
        "    if node is None:\n",
        "        return\n",
        "    # Recursively calculate cost complexity for each subtree\n",
        "    for link, child_node in node.linksDict.items():\n",
        "        self.cost_complexity_pruning(child_node)\n",
        "    # Calculate impurity for current subtree\n",
        "    impurity = self.calculate_impurity(node)\n",
        "    # Calculate number of leaves in current subtree\n",
        "    num_leaves = self.count_leaves(node)\n",
        "    # Calculate cost complexity parameter\n",
        "    cost_complexity = impurity + self.alpha * (num_leaves - 1)  # Subtract 1 to exclude the root node\n",
        "    node.cost_complexity = cost_complexity\n",
        "\n",
        "  def calculate_impurity(self, node):\n",
        "    if node is None:\n",
        "        return 0\n",
        "    total_samples = sum(node.class_distribution.values())\n",
        "    impurity = 1.0\n",
        "    for class_count in node.class_distribution.values():\n",
        "        proportion = class_count / total_samples\n",
        "        impurity -= proportion ** 2\n",
        "    return impurity\n",
        "\n",
        "  def count_leaves(self, node):\n",
        "        # Recursively count the number of leaves in the subtree\n",
        "        if node is None:\n",
        "            return 0\n",
        "        if node.isLeaf:\n",
        "            return 1\n",
        "        num_leaves = 0\n",
        "        for link, child_node in node.linksDict.items():\n",
        "            num_leaves += self.count_leaves(child_node)\n",
        "        return num_leaves\n",
        "import graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.3, random_state=42)\n",
        "\n",
        "tree = DecisionTree(tree=graphviz.Digraph())\n",
        "\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "test_predictions = tree.predict(X_test)\n",
        "\n",
        "y_val_pred = tree.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "print(\"Validation set accuracy:\", val_accuracy)\n",
        "\n",
        "# Calculate the accuracy on the testing set\n",
        "test_accuracy = (test_predictions == y_test).mean()\n",
        "print(\"Testing set accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "ttp8nlVdQFgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydot\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, attribute=None, threshold=None, left=None, right=None, value=None,mse=None):\n",
        "        self.attribute = attribute  # Splitting attribute\n",
        "        self.threshold = threshold  # Threshold for splitting\n",
        "        self.left = left  # Left child\n",
        "        self.right = right  # Right child\n",
        "        self.value = value  # Predicted value if it's a leaf node\n",
        "        self.mse = mse #squared error at each node\n",
        "def sum_squared_error(targets):\n",
        "    if len(targets) == 0:\n",
        "        return 0\n",
        "    mean = np.mean(targets)\n",
        "    return np.sum((targets - mean) ** 2)\n",
        "\n",
        "def construct_regression_tree(df, target_attribute, ats):\n",
        "    if len(ats) == 0:\n",
        "        # If no attributes left, return leaf node with mean target value\n",
        "        return Node(value=np.mean(df[target_attribute]))\n",
        "\n",
        "    best_score = float('inf')#representing integer as infinity in python\n",
        "    best_attribute = None\n",
        "    best_threshold = None\n",
        "\n",
        "    for x in ats:\n",
        "        if x is None:\n",
        "            continue  # Skip None attributes\n",
        "\n",
        "        # Calculate mean squared error for each attribute and its possible thresholds\n",
        "        values = df[x].values\n",
        "        thresholds = np.unique(values)\n",
        "        for y in thresholds:\n",
        "            left_examples = df[df[x] <= y]\n",
        "            right_examples = df[df[x] > y]\n",
        "\n",
        "            left_targets = left_examples[target_attribute].values\n",
        "            right_targets = right_examples[target_attribute].values\n",
        "\n",
        "            score = sum_squared_error(left_targets) + sum_squared_error(right_targets)\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                best_attribute = x\n",
        "                best_threshold = y\n",
        "\n",
        "    if best_attribute is None:\n",
        "        # If best_attribute is still None, return leaf node with mean target value\n",
        "        return Node(value=np.mean(df[target_attribute]))\n",
        "\n",
        "    left_examples = df[df[best_attribute] <= best_threshold]\n",
        "    right_examples = df[df[best_attribute] > best_threshold]\n",
        "\n",
        "    remaining_attributes = [x for x in ats if x != best_attribute]\n",
        "\n",
        "    left_subtree = construct_regression_tree(left_examples, target_attribute, remaining_attributes)\n",
        "    right_subtree = construct_regression_tree(right_examples, target_attribute, remaining_attributes)\n",
        "\n",
        "    return Node(attribute=best_attribute, threshold=best_threshold, left=left_subtree, right=right_subtree,mse=best_score)\n",
        "\n",
        "def visualize_regression_tree(node, graph=None):\n",
        "    if graph is None:\n",
        "        graph = pydot.Dot(graph_type='digraph')\n",
        "\n",
        "    if node.attribute is not None:#if it is not a leaf node\n",
        "        if node.value is not None:# If it has a value (e.g., for decision nodes)\n",
        "            label = f\"{node.attribute}\\n<= {node.threshold}\\nMSE: {node.value:.2f}\"\n",
        "        else:\n",
        "            label = f\"{node.attribute}\\n<= {node.threshold}\\nMSE: {node.mse:.2f}\"\n",
        "    else:\n",
        "        label = f\"Predicted value: {node.value:.2f}\"\n",
        "\n",
        "    node_name = str(id(node))# Unique node name using its memory address\n",
        "    graph.add_node(pydot.Node(node_name, label=label))# Add node to graph\n",
        "\n",
        "    if node.left is not None:\n",
        "        left_child_name = str(id(node.left))\n",
        "        #print(left_child_name)\n",
        "        graph.add_node(pydot.Node(left_child_name, label=\"\")) # Add placeholder for label update later\n",
        "        graph.add_edge(pydot.Edge(node_name, left_child_name, label=\"True\"))# Connect with edge\n",
        "        visualize_regression_tree(node.left, graph)# Recursively visualize subtree\n",
        "\n",
        "    if node.right is not None:\n",
        "        right_child_name = str(id(node.right))\n",
        "        graph.add_node(pydot.Node(right_child_name, label=\"\"))\n",
        "        graph.add_edge(pydot.Edge(node_name, right_child_name, label=\"False\"))\n",
        "        visualize_regression_tree(node.right, graph)\n",
        "\n",
        "    return graph\n",
        "\n",
        "    # Load dataset from CSV\n",
        "df = pd.read_csv(\"Reg_tree.csv\")\n",
        "\n",
        "    # Specify target attribute and attributes\n",
        "target_attribute = df.columns[-1]\n",
        "#\"Play\"\n",
        "ats = df.columns[0:4]\n",
        " #[\"Outlook\", \"Temperature\",\"Humidity\",\"Wind\"]\n",
        "\n",
        "    # Construct the regression tree\n",
        "tree = construct_regression_tree(df, target_attribute, ats)\n",
        "\n",
        "    # Visualize the regression tree\n",
        "graph = visualize_regression_tree(tree)\n",
        "graph.write_png('regression.png')\n"
      ],
      "metadata": {
        "id": "m9XdTJFiRxQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1.0 - x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 2 * np.random.random((n_inputs, n_neurons)) - 1\n",
        "        self.biases = 2 * np.random.random((1, n_neurons)) - 1\n",
        "\n",
        "    def calculate_output(self, input):\n",
        "        self.input = np.array(input)\n",
        "        weighted_sum = np.dot(self.input, self.weights) + self.biases\n",
        "        self.output = self.activation_function(weighted_sum)\n",
        "        return self.output\n",
        "\n",
        "    def calculate_delta(self, error, output):\n",
        "        return error * self.derivative_activation_function(output)\n",
        "\n",
        "    def update_weights(self, input, delta, learning_rate):\n",
        "        if delta.shape[0] != input.shape[0]:\n",
        "            input = input.reshape(1, -1)\n",
        "        self.weights += learning_rate * np.dot(delta.T, input).T\n",
        "        self.biases += learning_rate * np.sum(delta)\n",
        "\n",
        "    def activation_function(self, x):\n",
        "        return sigmoid(x)\n",
        "\n",
        "    def derivative_activation_function(self, x):\n",
        "        return sigmoid_derivative(x)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, layers):\n",
        "        layer = []\n",
        "        for i in range(0, len(layers) - 1):\n",
        "            layer.append(Layer(layers[i], layers[i + 1]))\n",
        "        self.layers = layer\n",
        "        self.itr = []\n",
        "        self.err = []\n",
        "\n",
        "    def predict(self, input):\n",
        "        output = input\n",
        "        for layer in self.layers:\n",
        "            output = layer.calculate_output(output)\n",
        "        return np.round(output,6)\n",
        "\n",
        "    def predict_(self, input):\n",
        "        output = input\n",
        "        for layer in self.layers:\n",
        "            output = layer.calculate_output(output)\n",
        "        return np.round(output)\n",
        "\n",
        "    def calculate_mse_error(self, expected_output, output):\n",
        "        return np.mean((expected_output - output) ** 2)\n",
        "\n",
        "    def calculate_binary_cross_entropy_error(self, expected_output, output):\n",
        "        return -np.mean(expected_output * np.log(output) + (1 - expected_output) * np.log(1 - output))\n",
        "\n",
        "    def calculate_categorical_cross_entropy_error(self, expected_output, output):\n",
        "        return -np.mean(expected_output * np.log(output))\n",
        "\n",
        "    def train(self, input, expected_output, learning_rate, error_function):\n",
        "        output = self.predict(input)\n",
        "        error = expected_output - output\n",
        "        for i in range(len(self.layers) - 1, -1, -1):\n",
        "            layer = self.layers[i]\n",
        "            input = layer.input\n",
        "            output = layer.output\n",
        "            if i == len(self.layers) - 1:\n",
        "                layer.delta = layer.calculate_delta(error, output)\n",
        "            else:\n",
        "                next_layer = self.layers[i + 1]\n",
        "                layer.delta = layer.calculate_delta(np.dot(next_layer.delta, next_layer.weights.T), output)\n",
        "            self.layers[i] = layer\n",
        "\n",
        "        for i in range(0, len(self.layers)):\n",
        "            layer = self.layers[i]\n",
        "            layer.update_weights(layer.input, layer.delta, learning_rate)\n",
        "            self.layers[i] = layer\n",
        "\n",
        "\n",
        "        if error_function == 'mse':\n",
        "            error = self.calculate_mse_error(expected_output, self.predict(input))\n",
        "        elif error_function == 'binary_cross_entropy':\n",
        "            error = self.calculate_binary_cross_entropy_error(expected_output, self.predict(input))\n",
        "        elif error_function == 'categorical_cross_entropy':\n",
        "            error = self.calculate_categorical_cross_entropy_error(expected_output, self.predict(input))\n",
        "\n",
        "        return error\n",
        "\n",
        "    def plot(self):\n",
        "        plt.plot(self.itr, self.err)\n",
        "        plt.title(\"Iterations vs Error\")\n",
        "        plt.xlabel(\"Iterations\")\n",
        "        plt.ylabel(\"Error\")\n",
        "\n",
        "    def plot1(self, lr, itr):\n",
        "        plt.plot(lr, itr)\n",
        "        plt.title(\"Learning Rate vs No of Iterations\")\n",
        "        plt.xlabel(\"Learning Rate\")\n",
        "        plt.ylabel(\"No of Iterations\")\n",
        "\n",
        "    def fit(self, e, lr, error_function):\n",
        "        for i in range(e):\n",
        "            for j in range(len(X)):\n",
        "                self.train(X[j], y[j], lr, error_function)\n",
        "            if i % 10000 == 0:\n",
        "                self.itr.append(i)\n",
        "                error = abs(float(y[0] - float(nn.predict(X[0]))))\n",
        "                self.err.append(error)\n",
        "                print(f\"Iteration: {i}, Error: {error}\")\n",
        "        final_error = abs(float(y[0] - float(nn.predict(X[0]))))\n",
        "        print(f\"Final Error: {final_error}\")\n",
        "\n",
        "X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n",
        "y = np.array([[0], [1], [1], [0], [1], [0], [0], [1]])\n",
        "nn = NeuralNetwork([3, 3, 1])\n",
        "\n",
        "nn.fit(100000, 0.7, 'binary_cross_entropy')\n",
        "print(nn.predict_(np.array([0, 0, 0])))\n",
        "print(nn.predict_(np.array([0, 0, 1])))\n",
        "print(nn.predict_(np.array([0, 1, 0])))\n",
        "print(nn.predict_(np.array([0, 1, 1])))\n",
        "print(nn.predict_(np.array([1, 0, 0])))\n",
        "print(nn.predict_(np.array([1, 0, 1])))\n",
        "print(nn.predict_(np.array([1, 1, 0])))\n",
        "print(nn.predict_(np.array([1, 1, 1])))\n",
        "\n",
        "nn.plot()\n"
      ],
      "metadata": {
        "id": "9w3_m331Sr8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_number = max(max(sublist) for sublist in X)\n",
        "result_list = [[element / max_number for element in sublist] for sublist in X]\n",
        "from sklearn.model_selection import train_test_split\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(result_list,y)\n",
        "import numpy as np\n",
        "class Layer:\n",
        "  def __init__(self,nNeurons,weights,bias,activation_function,activation_derivative):\n",
        "    self.nNeurons = nNeurons\n",
        "    self.weights = weights\n",
        "    self.bias = bias\n",
        "    self.activation_function = activation_function\n",
        "    self.activation_derivative = activation_derivative\n",
        "    self.delta = None\n",
        "  def calculate_output(self,data,input_Layer=False):\n",
        "    if(input_Layer):\n",
        "      self.output = data\n",
        "    else:\n",
        "      self.weighted_sum = np.dot(data,self.weights)+self.bias\n",
        "      self.output = np.array(self.activation_function(self.weighted_sum))\n",
        "    return self.output\n",
        "\n",
        "\n",
        "class Network:\n",
        "  def __init__(self,nLayers):\n",
        "    self.nLayers=nLayers\n",
        "    self.Layers=[]\n",
        "    self.pred_output=[]\n",
        "\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "    return 1/(1+np.exp(x))\n",
        "  def sigmoid_derivative(self,x):\n",
        "    return self.sigmoid(x)*(1-self.sigmoid(x))\n",
        "  def input_act(self,x):\n",
        "    return x\n",
        "  def relu(self,x):\n",
        "    return np.maximum(0,x)\n",
        "  def relu_derivative(self,x):\n",
        "    return np.where(x < 0, 0, 1)\n",
        "  def softmax(self,x):\n",
        "    exp_values=np.exp(x)\n",
        "    expSum=np.sum(exp_values)\n",
        "    return exp_values/expSum\n",
        "  def softmax_derivative(self,x):\n",
        "    return self.softmax(x) * (1 - self.softmax(x))\n",
        "\n",
        "\n",
        "\n",
        "  def Create_Network(self,nInputs):\n",
        "    for i in range(self.nLayers-1):\n",
        "      if i==0:\n",
        "        weights=np.ones(nInputs)\n",
        "        bias=np.ones((nInputs))\n",
        "        self.Layers.append(Layer(nInputs,weights,bias,self.input_act, self.input_act))\n",
        "      else:\n",
        "        nNeurons=int(input(f\"enter number of neurons of Layer {i+1} \"))\n",
        "        weights=0.1*np.random.randn(nInputs,nNeurons)\n",
        "        bias=0.1*np.random.randn((nNeurons))\n",
        "        self.Layers.append(Layer(nNeurons,weights,bias,self.relu, self.relu_derivative))\n",
        "        nInputs=nNeurons\n",
        "    nNeurons=int(input(f\"enter number of neurons in output layer \"))\n",
        "    weights=0.1*np.random.randn(nInputs,nNeurons)\n",
        "    bias=np.zeros((nNeurons))\n",
        "    self.Layers.append(Layer(nNeurons,weights,bias,self.softmax, self.softmax_derivative))\n",
        "\n",
        "\n",
        "  def forwardPass(self,Input_data):\n",
        "    for i in range(self.nLayers):\n",
        "      if(i==0):\n",
        "        Input_data=self.Layers[i].calculate_output(Input_data,True)\n",
        "      else:\n",
        "        Input_data=self.Layers[i].calculate_output(Input_data)\n",
        "    return Input_data\n",
        "\n",
        "\n",
        "  def calculate_deltas(self, targets):\n",
        "    for i in range(self.nLayers - 1, 0, -1):\n",
        "      if i == len(self.Layers)-1:\n",
        "        self.Layers[i].delta = (self.Layers[i].output - targets)\n",
        "      else:\n",
        "        self.Layers[i].delta = np.dot(self.Layers[i+1].delta,self.Layers[i+1].weights.T) * self.Layers[i].activation_derivative(self.Layers[i].weighted_sum)\n",
        "\n",
        "  def Update_Weights(self,lr):\n",
        "    for i in range(self.nLayers - 1, 0, -1):\n",
        "      self.Layers[i].bias -= np.dot(lr, self.Layers[i].delta)\n",
        "      self.Layers[i].weights -= self.Layers[i].delta[np.newaxis,:] * (np.dot(lr,self.Layers[i-1].output)[:, np.newaxis]  * self.Layers[i].weights)\n",
        "\n",
        "  def calculate_error(self, targets, outputs):\n",
        "      epsilon = 1e-15\n",
        "      outputs = np.clip(outputs, epsilon, 1 - epsilon)\n",
        "      loss = - np.sum(targets * np.log(outputs))\n",
        "      return loss\n",
        "\n",
        "  def backwardPass(self,targets, lr):\n",
        "    self.calculate_deltas(targets)\n",
        "    self.Update_Weights(lr)\n",
        "\n",
        "  def One_hot(self,x):\n",
        "    ans = []\n",
        "    for i in range(4):\n",
        "      if(i==x):\n",
        "        ans.append(1)\n",
        "      else:\n",
        "        ans.append(0)\n",
        "    return np.array(ans)\n",
        "\n",
        "  def fit(self, input_datas, target_labels, epochs, learning_rate):\n",
        "      for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for input_data,target in zip(input_datas, target_labels):\n",
        "          outputs = self.forwardPass(input_data)\n",
        "          targets = self.One_hot(target)\n",
        "          error = self.calculate_error(targets, outputs)\n",
        "          total_error+=error\n",
        "          self.backwardPass(targets, learning_rate)\n",
        "        print(f\"Epoch {epoch + 1} / {epochs}, Error: {total_error/len(input_datas)}\")\n",
        "\n",
        "nLayers=int(input(\"enter number of layers\"))\n",
        "\n",
        "obj=Network(nLayers)  #object for Network class\n",
        "obj.Create_Network(4096)  #creates a network by creating layers and appending to Layers list in Network class (this takes input length as argument)\n"
      ],
      "metadata": {
        "id": "5AE8408aTByi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pydot\n",
        "from IPython.display import Image, display\n",
        "class Neuron:\n",
        "    def __init__(self, weights, activation_function):\n",
        "        self.weights = weights\n",
        "        self.activation_function = activation_function\n",
        "        self.output = None\n",
        "\n",
        "    def compute_output(self, inputs):\n",
        "        weighted_sum = np.dot(inputs, self.weights)\n",
        "        self.output = self.activation_function(weighted_sum)\n",
        "        return self.output\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, num_neurons, num_inputs_per_neuron, activation_function):\n",
        "        self.neurons = [Neuron(np.random.rand(num_inputs_per_neuron), activation_function) for _ in range(num_neurons)]\n",
        "\n",
        "    def compute_outputs(self, inputs):\n",
        "        return [neuron.compute_output(inputs) for neuron in self.neurons]\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward_pass(self, input_data):\n",
        "        outputs = input_data\n",
        "        for layer in self.layers:\n",
        "            outputs = layer.compute_outputs(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def visualize(self):\n",
        "        graph = pydot.Dot(graph_type='digraph')\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            for j, neuron in enumerate(layer.neurons):\n",
        "                graph.add_node(pydot.Node(f'L{i}N{j}', label=f'Layer {i}\\nNeuron {j}\\n Value x({j}):{neuron.output:.2f}'))\n",
        "\n",
        "                if i > 0:\n",
        "                    for k, prev_neuron in enumerate(self.layers[i - 1].neurons):\n",
        "                        graph.add_edge(pydot.Edge(f'L{i-1}N{k}', f'L{i}N{j}', label=f'W={neuron.weights[k]:.2f}'))\n",
        "        print(graph)\n",
        "        graph.write(\"neural_network.dot\")\n",
        "        graph.write_png('neural_network.png')\n",
        "        display(Image('neural_network.png'))\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Create layers with appropriate number of neurons and inputs per neuron\n",
        "input_layer = Layer(num_neurons=3, num_inputs_per_neuron=3, activation_function=sigmoid)\n",
        "hidden_layer = Layer(num_neurons=4, num_inputs_per_neuron=3, activation_function=sigmoid)\n",
        "hidden_layer1 = Layer(num_neurons=4, num_inputs_per_neuron=4, activation_function=sigmoid)\n",
        "output_layer = Layer(num_neurons=2, num_inputs_per_neuron=4, activation_function=sigmoid)\n",
        "\n",
        "# Create a neural network and add layers\n",
        "neural_network = NeuralNetwork()\n",
        "neural_network.add_layer(input_layer)\n",
        "neural_network.add_layer(hidden_layer)\n",
        "neural_network.add_layer(hidden_layer1)\n",
        "neural_network.add_layer(output_layer)\n",
        "\n",
        "# Example input data\n",
        "input_data = np.array([0.5, 0.3, 0.1])\n",
        "\n",
        "output = neural_network.forward_pass(input_data)\n",
        "print(\"Final Output:\", output)\n",
        "\n",
        "neural_network.visualize()"
      ],
      "metadata": {
        "id": "AI1boEIsTZqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Neuron:\n",
        "  type:str=None\n",
        "  weights:list=field(default_factory=list)\n",
        "  inputs:list=field(default_factory=list)\n",
        "  outputs:list=field(default_factory=list)\n",
        "  activationfunc=sigmoid\n",
        "  DEbyDX=None\n",
        "  DEbyDY=None\n",
        "  def computeipoutput(self):\n",
        "        if self.type==\"input\":\n",
        "          self.weights=None\n",
        "          self.inputs=None\n",
        "  def computeoutput(self):\n",
        "    pass\n",
        "@dataclass\n",
        "class Layer:\n",
        "  type:str=None\n",
        "  listofneurons:list=field(default_factory=list)\n",
        "  weights:list=field(default_factory=list)\n",
        "  #inputs:list=field(default_factory=list)\n",
        "  outputs:list=field(default_factory=list)\n",
        "  activationfunc:sigmoid=sigmoid\n",
        "  def computeoutput(self,input):\n",
        "    if self.type!=\"input\":\n",
        "      netsum=np.dot(input,self.weights)\n",
        "      print(netsum)\n",
        "      self.outputs=self.activationfunc(netsum)\n",
        "      return self.outputs\n",
        "    else:\n",
        "        self.outputs=input\n",
        "@dataclass\n",
        "class Network:\n",
        "  listoflayers:list=field(default_factory=list)\n",
        "  def forwardpass(self,input):\n",
        "    for i in range(len(self.listoflayers)-1):\n",
        "      weights=np.ones((len(self.listoflayers[i].listofneurons),len(self.listoflayers[i+1].listofneurons)))\n",
        "      self.listoflayers[i+1].weights=weights\n",
        "\n",
        "    self.listoflayers[0].computeoutput(input)\n",
        "    self.listoflayers[1].inputs=input\n",
        "    activation=input\n",
        "    for i in range(1,len(self.listoflayers)):\n",
        "      print(activation)\n",
        "      activation=self.listoflayers[i].computeoutput(activation)\n",
        "    print(activation)\n",
        "MLP=Network()\n",
        "ipneurons=int(input(\"enter no of i/p neurons\"))\n",
        "iplayer=Layer(type=\"input\")\n",
        "for i in range(ipneurons):\n",
        "    neuron=Neuron(type=\"input\")\n",
        "    iplayer.listofneurons.append(Neuron)\n",
        "MLP.listoflayers.append(iplayer)\n",
        "\n",
        "hidlay=int(input(\"enter no of hidden layers\"))\n",
        "for i in range(hidlay):\n",
        "   hiddenlayer=Layer()\n",
        "   hidneu=int(input(\"enter no of hidden neurons\"))\n",
        "   for i in range(hidneu):\n",
        "     neuron=Neuron()\n",
        "     hiddenlayer.listofneurons.append(Neuron)\n",
        "   MLP.listoflayers.append(hiddenlayer)\n",
        "\n"
      ],
      "metadata": {
        "id": "10a2PC8DTzAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLP=Network()\n",
        "ipneurons=int(input(\"enter no of i/p neurons\"))\n",
        "iplayer=Layer(type=\"input\")\n",
        "for i in range(ipneurons):\n",
        "    neuron=Neuron(type=\"input\")\n",
        "    iplayer.listofneurons.append(Neuron)\n",
        "MLP.listoflayers.append(iplayer)\n",
        "\n",
        "hidlay=int(input(\"enter no of hidden layers\"))\n",
        "for i in range(hidlay):\n",
        "   hiddenlayer=Layer()\n",
        "   hidneu=int(input(\"enter no of hidden neurons\"))\n",
        "   for i in range(hidneu):\n",
        "     neuron=Neuron()\n",
        "     hiddenlayer.listofneurons.append(Neuron)\n",
        "   MLP.listoflayers.append(hiddenlayer)\n",
        "@dataclass\n",
        "class Layer:\n",
        "    noofneurons:int=None\n",
        "    neurons:list=field(default_factory=list)\n",
        "    weights:np.array=None\n",
        "    typ:str=None\n",
        "    input:np.array=None\n",
        "    output:np.array=None\n",
        "    def initial(self,typ=None,prevlayer=None):\n",
        "        if prevlayer==None:\n",
        "                for i in range(self.noofneurons):\n",
        "                    n=Neuron(typ=self.typ)\n",
        "                    self.neurons.append(n)\n",
        "\n",
        "        else:\n",
        "            for i in range(self.noofneurons):\n",
        "                    n=Neuron(typ=typ)\n",
        "                    self.neurons.append(n)\n",
        "\n",
        "            #for n in prevlayer.neurons:\n",
        "                #n.weights=np.ones((len(self.neurons),))\n",
        "                #n.weightsin=np.ones((len(self.neurons),))\n",
        "            self.weights=np.ones((len(prevlayer.neurons),len(self.neurons)))\n",
        "    def computeoutput(self,inputs):\n",
        "        return [neuron.computeoutput(inputs) for neuron in self.neurons]\n",
        "@dataclass\n",
        "class NeuralNetwork:\n",
        "    layers:list=field(default_factory=list)\n",
        "    def add(self,layer,typ):\n",
        "        if typ=='input':\n",
        "            self.layers.append(layer)\n",
        "            layer.initial(typ='input')\n",
        "        if typ=='hidden' or  typ=='output':\n",
        "            self.layers.append(layer)\n",
        "            layer.initial(typ,self.layers[-2])\n",
        "    def forwardpass(self,inputdata):\n",
        "        self.layers[0].input= inputdata\n",
        "        self.layers[0].output= inputdata\n",
        "        output=self.layers[0].output\n",
        "        for i in range(1,len(self.layers)):\n",
        "            weightedsum=np.dot(self.layers[i].weights.T,output)\n",
        "            self.layers[i].input=weightedsum\n",
        "            self.layers[i].output=sigmoid(weightedsum)\n",
        "            output=self.layers[i].output\n",
        "        return output\n",
        "n=NeuralNetwork()\n",
        "n.add(Layer(1),typ='input')\n",
        "n.add(Layer(2),typ='hidden')\n",
        "n.add(Layer(2),typ='hidden')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CoGNp4EZUG46"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}